{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Packaging Machine Learning models involves saving it and wrapping it with an API. Following are the steps in packaging an ML model:\n",
        "\n",
        "Step 1: Export/Save the Trained Model\n",
        "\n",
        "Step 2: Writing a Wrapper Function to Load the Model\n",
        "\n",
        "Step 3: Setting Up an API to Serve the Model"
      ],
      "metadata": {
        "id": "uHH-TfI9Rr0v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OE--GhuuRXHO"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step1: Export/Save the trained Model"
      ],
      "metadata": {
        "id": "-haqvYpsT1_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load dataset\n",
        "iris=load_iris()\n",
        "X_train,X_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.3)\n",
        "\n",
        "#Train model\n",
        "model=RandomForestClassifier()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "#Save Model\n",
        "joblib.dump(model,'rf_model.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tgl_dC5gTUFe",
        "outputId": "3b4d3558-ac77-44ac-dfe5-424c297b8639"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rf_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step2: Writing a Wrapper Function to Load the Model\n",
        "\n",
        "For deployment, weâ€™ll need a function that loads the model and makes predictions. This wrapper function will be exposed through an API or used in the production environment. Following are the steps to write the wrapper function:"
      ],
      "metadata": {
        "id": "BYtm7KFXT8Xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to load the saved model\n",
        "def load_model():\n",
        "  model=joblib.load('rf_model.pkl')\n",
        "  return model\n",
        "\n",
        "#function to make predictions\n",
        "def predict(input_data):\n",
        "  model=load_model()\n",
        "  prediction=model.predict(input_data)\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "9xjPP5T8TwLe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Setting Up an API to Serve the Model\n",
        "\n",
        "To serve the model in production, we can expose it via an API. Flask or FastAPI is often used to create REST APIs for this purpose."
      ],
      "metadata": {
        "id": "OvJXgYgWIgV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "\n",
        "app=Flask(__name__)\n",
        "\n",
        "#load the model\n",
        "model=joblib.load('rf_model.pkl')\n",
        "\n",
        "#define a route for making predictions\n",
        "@app.route('/predict',methods=['POST'])\n",
        "def predict():\n",
        "  data=request.get_json() #Get input data from POST request\n",
        "  input_data=np.array(data['input']).reshape(1,-1)\n",
        "  prediction=model.predict(input_data)\n",
        "  return jasonify({'prediction':int(prediction[0])})\n",
        "\n",
        "if __name__=='__main__':\n",
        "  app.run(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gcTpqsrIfJ6",
        "outputId": "42f3720a-7772-403c-8d61-fa3b10d1da6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use Postman to test this API by giving input in JSON format, to what the model requires"
      ],
      "metadata": {
        "id": "a-iBhm0TQn44"
      }
    }
  ]
}